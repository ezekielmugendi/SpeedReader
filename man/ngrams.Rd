% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ngrams.R
\name{ngrams}
\alias{ngrams}
\title{Runs Stanford CoreNLP on a collection of documents}
\usage{
ngrams(tokenized_documents = NULL, tokenized_documents_directory = NULL,
  file_list = NULL, ngram_lengths = c(1, 2, 3), remove_punctuation = TRUE,
  remove_numeric = TRUE, JK_filtering = FALSE, verb_filtering = FALSE,
  phrase_extraction = FALSE, return_tag_patterns = FALSE)
}
\arguments{
\item{tokenized_documents}{An optional list object output by the corenlp()
or corenlp_blocked() functions containing tokenized document dataframes (one
per document).}

\item{tokenized_documents_directory}{An optional directory path to a
directory contianing CoreNLP_Output_x.Rdata files output by the corenlp()
or corenlp_blocked() functions. Cannot be supplied in addition to the
'tokenized_documents' argument.}

\item{file_list}{An optional list of CoreNLP_Output_x.Rdata  files to be used
if tokenized_documents_directory option is specified. Can be useful if the
user only wants to process a subset of documents in the directory such as
when the corpus is extremely large.}

\item{ngram_lengths}{A vector of N-gram lengths (in tokens) to be returned by
the function. Defaults to c(1,2,3) which returns all unigrams, bigrams and
trigrams.}

\item{remove_punctuation}{Removes any N-Grams with atleast one token
containing one or more punctuation characters: [!"#$%&'()*+,\-./:;<=>?@[\\\]
^_`{|}~]}

\item{remove_numeric}{Removes any N-Grams with atleast one token containing
one or more numerals (0-9).}

\item{JK_filtering}{Defaults to FALSE. If TRUE then bigrams and trigrams will be
extracted, and filtered according to the tag patterns described in Justeson,
John S., and Slava M. Katz. "Technical terminology: some linguistic properties
and an algorithm for identification in text." Natural language engineering
1.01 (1995): 9-27. Available: https://brenocon.com/JustesonKatz1995.pdf.
The POS tag patterns used are: AN, NN, AAN, ANN, NAN, NNN, NPN. Note that
this method requires input produced by the corenlp() or corenlp_blocked()
functions.}

\item{verb_filtering}{Defaults to FALSE. If TRUE, then short verb phrases
will be extracted in a manner similar to that described in JK_filtering
above. The POS tag patterns used are: VN, VAN, VNN, VPN, ANV, VDN. Note that
this method requires input produced by the corenlp() or corenlp_blocked()
functions.}

\item{phrase_extraction}{Defaults to FALSE. If TRUE, then full phrases of
arbitrary length will be extracted following the procedure described in Denny,
O'Connor, and Wallach (2016). This method will produce the most phrases, of
highest quality, but will take significantly longer than other methods. Not
currently implemented. Note that this method requires input produced by the
corenlp() or corenlp_blocked() functions.}

\item{return_tag_patterns}{Defaults to FALSE. If TRUE and either JK_filtering
= TRUE, verb_filtering = TRUE, or phrase_extraction = TRUE, then the tag
pattern matched in forming the n-gram/phrase will be returned as an
accompanying vector.}
}
\value{
Returns a list of lists (one list per document) with entries for n-grams
of each size specified in the ngram_lengths argument. May also return metadata
if return_tag_patterns = TRUE.
}
\description{
Runs Stanford CoreNLP on a collection of documents
}
\examples{
\dontrun{
directory <- system.file("extdata", package = "SpeedReader")[1]
Ngrams <- ngrams()
}
}

