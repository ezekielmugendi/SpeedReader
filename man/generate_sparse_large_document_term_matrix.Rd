% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/sparse_large_document_term_matrix.R
\name{generate_sparse_large_document_term_matrix}
\alias{generate_sparse_large_document_term_matrix}
\title{A function to generate a sparse large document term matrix in blocks from a list document term vector lists stored as .Rdata object on disk. This function is designed to work on very large corpora (up to 10's of billions of words) that would otherwise be computationally intractable to generate a document term matrix for using standard methods. However, this function, and R itself, is limited to a vocaublary size of roughly 2.1 billion unique words.}
\usage{
generate_sparse_large_document_term_matrix(file_directory, file_list,
  aggregate_vocabulary = NULL, maximum_vocabulary_size = -1)
}
\arguments{
\item{file_directory}{The directory where you have stored a series of intermediate .Rdata files, each of which contains an R list object named "document_term_vector_list" which is a list of document term vectors. This can most easily  be generated by the generate_document_term_vector_list() function.}

\item{file_list}{A list of intermediate files prefferably generated by the generate_document_term_vector_list() function, that reside in the file_directory.}

\item{aggregate_vocabulary}{If we already know the aggregate vocabulary, then it can be provided as a string vector. When providing this vector it will be mush more computationally efficient to provide it order from most frequently appearing words to least frequently appearing ones for computational efficiency. Defaults to NULL in which case the vocabulary will be determined inside the function.}

\item{maximum_vocabulary_size}{An integer specifying the maximum number of unique word types you expect to encounter. Defaults to -1 in which case the maximum vocabulary size used for pre-allocation in finding the common vocabular across all documents will be set to approximately the number of words in all documents. If you beleive this number to be over 2 billion, or are memory limited on your computer it is recommended to set this to some lower number. For normal english words, a value of 10 million should be sufficient. If you are dealing with n-grams then somewhere in the neighborhood of 100 million to 1 billion is often appropriate. If you have reason to believe that your final vocabulary size will be over ~2,147,000,000 then you should considder working in C++ or rolling your own functions, and congratuations, you have really large text data.}
}
\value{
A sparse document term matrix object. This will likely still be a large file.
}
\description{
A function to generate a sparse large document term matrix in blocks from a list document term vector lists stored as .Rdata object on disk. This function is designed to work on very large corpora (up to 10's of billions of words) that would otherwise be computationally intractable to generate a document term matrix for using standard methods. However, this function, and R itself, is limited to a vocaublary size of roughly 2.1 billion unique words.
}

