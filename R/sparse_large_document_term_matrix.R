#' A function to generate a sparse large document term matrix in blocks from a list document term vector lists stored as .Rdata object on disk. This function is designed to work on very large corpora (up to 10's of billions of words) that would otherwise be computationally intractable to generate a document term matrix for using standard methods. However, this function, and R itself, is limited to a vocaublary size of roughly 2.1 billion unique words.
#'
#' @param file_directory The directory where you have stored a series of intermediate .Rdata files, each of which contains an R list object named "document_term_vector_list" which is a list of document term vectors. This can most easily  be generated by the generate_document_term_vector_list() function.
#' @param file_list A list of intermediate files prefferably generated by the generate_document_term_vector_list() function, that reside in the file_directory.
#' @param aggregate_vocabulary If we already know the aggregate vocabulary, then it can be provided as a string vector. When providing this vector it will be mush more computationally efficient to provide it order from most frequently appearing words to least frequently appearing ones for computational efficiency. Defaults to NULL in which case the vocabulary will be determined inside the function.
#' @param maximum_vocabulary_size An integer specifying the maximum number of unique word types you expect to encounter. Defaults to -1 in which case the maximum vocabulary size used for pre-allocation in finding the common vocabular across all documents will be set to approximately the number of words in all documents. If you beleive this number to be over 2 billion, or are memory limited on your computer it is recommended to set this to some lower number. For normal english words, a value of 10 million should be sufficient. If you are dealing with n-grams then somewhere in the neighborhood of 100 million to 1 billion is often appropriate. If you have reason to believe that your final vocabulary size will be over ~2,147,000,000 then you should considder working in C++ or rolling your own functions, and congratuations, you have really large text data.
#' @param using_document_term_counts Defaults to FALSE, if TRUE then we epect a document_term_count_list for each chunk. See generate_document_term_matrix() for more information.
#' @return A sparse document term matrix object. This will likely still be a large file.
#' @export
generate_sparse_large_document_term_matrix <- function(file_directory,
                                              file_list,
                                              aggregate_vocabulary = NULL,
                                              maximum_vocabulary_size = -1,
                                              using_document_term_counts = FALSE){
    # get the current working directory so we can change back to it.
    current_directory <- getwd()
    # change working directory file_directory
    setwd(check_directory_name(file_directory))
    # get a count of the number of intermediate files
    num_files <- length(file_list)
    if(num_files < 2){
        stop("This function expects N > 1 intermediate files in file_list. Either split this file into smaller constituents or use the generate_document_term_matrix() function.")
    }

    if(!using_document_term_counts){
        document_term_count_list = NULL
    }
    # otherwise we expect an object named document_term_count_list

    # if the user did not provide a vocabulary, then we have to generate one.
    if(is.null(aggregate_vocabulary)){
        load(file_list[1])
        vocab <- count_words(document_term_vector_list,
                             maximum_vocabulary_size,
                             existing_vocabulary = NULL,
                             existing_word_counts = NULL,
                             document_term_count_list = document_term_count_list)
        for(i in 2:num_files){
            load(file_list[i])
            # If we are approaching the maximum vocabulary size then increase it by 50%
            if(vocab$total_unique_words/maximum_vocabulary_size > 0.8){
                maximum_vocabulary_size <- 1.5*maximum_vocabulary_size
            }
            vocab <- count_words(document_term_vector_list,
                        maximum_vocabulary_size,
                        existing_vocabulary = vocab$unique_words,
                        existing_word_counts = vocab$word_counts,
                        document_term_count_list = document_term_count_list)
        }
        aggregate_vocabulary <- vocab$unique_words
    }

    # now get the aggregate vocabulary size
    aggregate_vocabulary_size <- length(aggregate_vocabulary)

    #loop over bill blocks to add to matricies
    for(i in 1:length(num_files)){
        cat("Loading Document Block Number:",i,"\n")
        load(file_list[i])

        current_document_lengths <- unlist(lapply(document_term_vector_list, length))

        cat("Total Terms in Block:",sum(current_document_lengths),"\n")

        current_dw <- generate_document_term_matrix(document_term_vector_list,
                                                    aggregate_vocabulary,
                                                    document_term_count_list)

        #turn into simple triplet matrix and rbind to what we already have
        current_dw <- slam::as.simple_triplet_matrix(current_dw)
        if(i == 1){
            sparse_document_term_matrix <- current_dw
        }else{
            sparse_document_term_matrix <- rbind(sparse_document_term_matrix,
                                                 current_dw)
        }
    }

    #get the names right
    colnames(sparse_document_term_matrix) <- aggregate_vocabulary

    return(sparse_document_term_matrix)
}
