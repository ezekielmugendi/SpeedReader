#' A function to generate a sparse large document term matrix in blocks from a list document term vector lists stored as .Rdata object on disk. This function is designed to work on very large corpora (up to 10's of billions of words) that would otherwise be computationally intractable to generate a document term matrix for using standard methods. However, this function, and R itself, is limited to a vocaublary size of roughly 2.1 billion unique words.
#'
#' @param file_list A character vector of paths to intermediate files prefferably generated by the generate_document_term_vector_list() function, that reside in the file_directory or have their full path specified.
#' @param file_directory The directory where you have stored a series of intermediate .Rdata files, each of which contains an R list object named "document_term_vector_list" which is a list of document term vectors. This can most easily  be generated by the generate_document_term_vector_list() function. Defaults to NULL, in which case the current working directory will be used. This argument can also be left as NULL if the full path to the intermediate files you are using is provided.
#' @param aggregate_vocabulary If we already know the aggregate vocabulary, then it can be provided as a string vector. When providing this vector it will be mush more computationally efficient to provide it order from most frequently appearing words to least frequently appearing ones for computational efficiency. Defaults to NULL in which case the vocabulary will be determined inside the function.
#' @param maximum_vocabulary_size An integer specifying the maximum number of unique word types you expect to encounter. Defaults to -1 in which case the maximum vocabulary size used for pre-allocation in finding the common vocabular across all documents will be set to approximately the number of words in all documents. If you beleive this number to be over 2 billion, or are memory limited on your computer it is recommended to set this to some lower number. For normal english words, a value of 10 million should be sufficient. If you are dealing with n-grams then somewhere in the neighborhood of 100 million to 1 billion is often appropriate. If you have reason to believe that your final vocabulary size will be over ~2,147,000,000 then you should considder working in C++ or rolling your own functions, and congratuations, you have really large text data.
#' @param using_document_term_counts Defaults to FALSE, if TRUE then we epect a document_term_count_list for each chunk. See generate_document_term_matrix() for more information.
#' @param generate_sparse_term_matrix Defaults to TRUE. If FALSE, then the function only generates and saves the aggregate vocabulary (and counts) in the form of a list object named Aggregate_Vocabular_and_Counts.Rdata in file_directory or the current working directory if file_directory = NULL. This option is useful if we have an extremely large corpus and may wnat to trim the vocabulary first before providing an aggregate_vocabulary.
#' @param parallel Defaults to FALSE, but can be set to TRUE to speed up processing provided the machine hte user is using has enough RAM. Parallelization is currently implemented using forking in the parallel package (mclapply) so it will only work on UNIX based platforms..
#' @param cores Defaults to 1. Can be set to the number of cores on your computer.
#' @return A sparse document term matrix object. This will likely still be a large file.
#' @export
generate_sparse_large_document_term_matrix <- function(file_list,
                                              file_directory = NULL,
                                              aggregate_vocabulary = NULL,
                                              maximum_vocabulary_size = -1,
                                              using_document_term_counts = FALSE,
                                              generate_sparse_term_matrix = TRUE,
                                              parallel = FALSE,
                                              cores = 1){
    # get the current working directory so we can change back to it.
    current_directory <- getwd()
    # change working directory file_directory
    if(!is.null(file_directory)){
        setwd(check_directory_name(file_directory))
    }
    # get a count of the number of intermediate files
    num_files <- length(file_list)
    cat("Generating sparse document term matrix from",num_files,"blocks.\n")
    if(num_files < 2){
        stop("This function expects N > 1 intermediate files in file_list. Either split this file into smaller constituents or use the generate_document_term_matrix() function.")
    }

    # set global variable to NULL
    document_term_vector_list = NULL

    if(!using_document_term_counts){
        document_term_count_list = NULL
    }
    # otherwise we expect an object named document_term_count_list

    # if the user did not provide a vocabulary, then we have to generate one.
    if(is.null(aggregate_vocabulary)){
        load(file_list[1])
        cat("Generating vocabulary from block 1 ...\n")
        vocab <- count_words(document_term_vector_list,
                             maximum_vocabulary_size = maximum_vocabulary_size,
                             existing_vocabulary = NULL,
                             existing_word_counts = NULL,
                             document_term_count_list = document_term_count_list)
        for(i in 2:num_files){
            load(file_list[i])
            cat("Generating vocabulary from block",i,"...\n")
            # If we are approaching the maximum vocabulary size then increase it by 50%
            if(maximum_vocabulary_size != -1){
                if(vocab$total_unique_words/maximum_vocabulary_size > 0.8){
                    maximum_vocabulary_size <- 1.5*maximum_vocabulary_size
                }
            }
            vocab <- count_words(document_term_vector_list,
                        maximum_vocabulary_size= maximum_vocabulary_size,
                        existing_vocabulary = vocab$unique_words,
                        existing_word_counts = vocab$word_counts,
                        document_term_count_list = document_term_count_list)
        }
        aggregate_vocabulary <- vocab$unique_words
    }

    # now get the aggregate vocabulary size
    aggregate_vocabulary_size <- length(aggregate_vocabulary)
    cat("Aggregate vocabulary size:",aggregate_vocabulary_size,"\n")

    if(!generate_sparse_term_matrix){
        Aggregate_Vocabular_and_Counts <- vocab
        save(Aggregate_Vocabular_and_Counts,
             file = "Aggregate_Vocabular_and_Counts.Rdata")
    }

    if(generate_sparse_term_matrix){
        if(!parallel){
            #loop over bill blocks to add to matricies
            for(j in 1:num_files){
                cat("Generating sparse matrix from block number:",j,"\n")
                load(file_list[j])

                current_document_lengths <- unlist(lapply(document_term_vector_list, length))

                cat("Total terms in current block:",sum(current_document_lengths),"\n")

                current_dw <- generate_document_term_matrix(document_term_vector_list,
                                                            vocabulary = aggregate_vocabulary,
                                                            document_term_count_list = document_term_count_list)

                #turn into simple triplet matrix and rbind to what we already have
                current_dw <- slam::as.simple_triplet_matrix(current_dw)
                if(j == 1){
                    sparse_document_term_matrix <- current_dw
                }else{
                    sparse_document_term_matrix <- rbind(sparse_document_term_matrix,
                                                         current_dw)
                }
            }
        }else{
            # if we are using parallel
            chunks <- ceiling(num_files/cores)
            counter <- 1
            start <- 1
            end <- min(cores,num_files)
            for(j in 1:chunks){
                # get indexing right
                cat("Currently working on files:",start, "to",end,"of", num_files,"\n")
                current_file_indexes <- start:end
                start <- start + cores
                end <- min(end + cores,num_files)

                cur_files <- file_list[current_file_indexes]
                cat("Applying Across Cluster ... \n")
                result <- parallel::mclapply(X = cur_files,
                                             FUN = sparse_doc_term_parallel,
                                             vocab = aggregate_vocabulary,
                                             mc.cores = cores)
                cat("Cluster apply complete ... \n")
                for(k in 1:length(result)){
                    cat("Adding current block",k,"of",length(result),"to sparse matrix ... \n")
                    if(counter == 1){
                        sparse_document_term_matrix <- result[[k]]
                    }else{
                        sparse_document_term_matrix <- rbind(sparse_document_term_matrix,
                                                             result[[k]])
                    }
                    counter <- counter + 1
                }
            }
        }
        #reset working directory
        setwd(current_directory)

        #get the names right
        colnames(sparse_document_term_matrix) <- aggregate_vocabulary

        return(sparse_document_term_matrix)
    }
}
